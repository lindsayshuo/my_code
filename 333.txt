TFLMS
论文：https://arxiv.org/pdf/1807.02037.pdf

https://github.com/IBM/tensorflow-large-model-support
https://blog.csdn.net/Crisholic/article/details/106660501


一、原理
1）工作原理
2）做swap in和swap out的条件

二、实验
 1、tensorflow 1.5+TFLMSv1版本，选择resnet or bert （用nvidia官网脚本https://github.com/NVIDIA/DeepLearningExamples）,打开、关闭情况下对比
  1）使能方法
  2) tensorboard 图上看swapin和out算子插入位置
  3) 相同batch下对比性能和内存占用（包括gpu和host）的变化
  4）最大batch 的对比
 2、tensorflow 1.5+TFLMSv2版本，和上面做类似的验证 
 3、tensorflow 2+v2版本，和上面做类似的验证 


https://devblogs.nvidia.com/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/


本周任务报告：
算法研究
1、mxnet recomputation 机制研究工作的任务总结（已完成）
2、SWAP机制论文的阅读，tf large model support的调研，conda环境下tflms及tf2.1的配置（进行中）
3、tf2.1 环境下resnet50 的性能测试。（进行中）
4、conda环境下打开tflms开关下，tf2.1的resnet50性能测试。（进行中）

竞品测试：
Tf1.14  resnet50 256 在v100*8上的性能测试，目前测试每个epoch约12min,和官网提供数据相差较大，正在查找原因。
https://pan.baidu.com/s/1MGImmJxyvYH2RYJlqtyEQw?errno=0&errmsg=Auth%20Login%20Sucess&&bduss=&ssnerror=0&traceid=
