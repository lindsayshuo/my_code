深度学习显存优化技术调研报告
 随着深度学习在各领域越来越广泛的应用，新的算法模型不断涌现。因为模型或者训练数据占用空间过大，超过显存空间，导致模型无法在显卡上训练的情况时有发生。采用显存优化算法，在尽量减少性能的损耗的基础上，使显存可以支持大模型以及训练数据进行训练，是显存优化技术的解决的主要问题。
在华为合作方的指导下，作者对显存优化的常用技术手段，进行了调研和实测，并将实测结果和官方发布性能指标进行了对比和分析。最后，对显存优化领域内的各类算法的优缺点进行了总结，并对内存优化算法的发展趋势进行了展望。
现将研究成果汇总如下：
一、显存优化算法简介
根据相关论文的调研，当前业界常用的显存优化方式有4种，分别为Live Analysis(即资源回收，回收无用tensor的显存)、LRU(将使用较少的tensor暂存到主机内存或等其他存储设备中)、Recomputation(对于占用显存多、计算消耗小的tensor不进行存储，需要使用时重新计算)以及SWAP(基于通用数据流图的显存显存优化策略，将主机内存作为扩展内存池来克服显存的内存限制)。
二、各类显存优化方法分析
2.1  Live Analysis
基于计算图的算法对于后续操作不依赖的tensor进行回收，释放相应的显存空间。算法主要是对N层网络分别进行计算，每层计算结束后加入当前层输出 ，如果输出中的张量与后续的计算无关，则进行删除释放内存。可以看出，Live Analysis可以在不影响任何性能的基础上，降低显存使用空间。该算法已为多数深度学习框架使用。
2.2 LRU
在空间不足时，将所有计算过并且对当前层计算无用的张量暂存到CPU中。选取暂存tensor的策略为Least Recent Used (LRU)，即最长时间未被使用的tensor将被暂存到CPU内存中。
由于CPU和显存之间的数据转移会影响计算效率，在转移过程中，多次的传输往往会带来时间损耗，所以目前这种方法使用的较少。
2.3 Recomputation
对于POOL，ACT，LRN和BN等层，其输出占用较多的显存，但是计算起来却很简单。可以在每次使用之前重新计算相应层的输出，从而节约显存。
因为不是对所有层均进行重计算，所以可以对需要重计算的层汇总为一个个区块，称为recomputation segment。在每个区块内，存在两种重计算的策略：speed-centric和memory -centric，即速度优先和显存优先。速度优先存储每一层重计算的结果，直至不再需要或者区块结束，从而仅需要对每一层进行一次重计算，计算复杂度为O(N)，多存储的tensor数为N（其中N为该recomputation segment内的神经层数）。memory优先则是每次仅存储重计算的输出，而不存储中间结。若中间结果在后续计算中被依赖，则在需求时重新计算,其计算复杂度为 。
2.3.1 Memonger内存优化算法简介
memonger作为recomputation算法较早形成完整方案的一个公开模块，被pytorch、Mxnet以及paddlepaddle等众多框架沿用。
Memonger是一种减少深度神经网络训练时内存消耗的系统性方法。算法仅耗费 的内存就可以训练一个 n层网络并且使得每个mini-batch只需要一个额外的前向计算成本。
通用方法的一个快速应用是丢弃低成本操作的结果并保持计算耗时的结果。这通常用于卷积神经网络中的Conv-BatchNorm-Activation流水线。在实践中，这将节省内存，并且计算开销很小，因为批量归一化和激活函数的计算成本很低。
假设我们将 n层网络划分为 k段，训练此网络的内存成本如下给出。
	 	（1）
等式的第一部分是在每个段上反向传播的内存开销。鉴于每段是平等分配的，这将转化为 的成本。等式的第二部分是存储分段之间的中间输出的成本。设置 ，我们得到 的成本。该算法只需在训练过程中增加一次前向，但将内存开销降低为次线性。由于后向操作几乎是前向的两倍，所以只会稍许减慢计算速度。
一般在多数情况下，每层的内存成本是不一样的，所以我们不能简单地设置  。但是，中间输出和每个阶段成本之间的权衡仍然存在。在这种情况下，给定每个段内的内存开销预算作为单一参数B 。通过贪婪算法进行分配，通过给不同的节点分配内存B，然后在依次计算实际内存，最后在每个节点中把最大的内存设置为新的节点内存用于内存开销。要么给中间输出分配更多的内存，要么给每个阶段分配更多的计算。当我们进行静态内存分配时，给定每个分配计划，我们可以得到精确的内存消耗。我们可以使用这些信息对 B 进行启发式搜索，以找到平衡两者成本的最佳内存方案。
可以将每个段视为一个块操作符，它将段中的所有操作组合在一起。这一思路如图2.3.1.1所示。组合运算符在描述其内部计算的子图上运行来计算梯度。这一观点允许我们将一系列操作视为子程序。子图中的优化不会影响外部运算。因此，我们可以递归地将我们的内存优化方案应用于每个子图。
 
图2.3.1.1 内存分配优化的递归视图
图中每一段可以看作是一个单一的运算符，它将段内的所有运算符组合在一起。在每个运算符中，执行一个子图来计算梯度。
用递归进一步减少内存，设 g(n)为在 n 层神经网络上进行前向和后向传播的内存代价。假设我们将 k个中间结果存储在图中，并在子路径上进行前向和后向传播时递归地应用相同的策略。有以下递归公式。
	 
（2）
求解这个递归公式得到
	 
（3）
设 ，得到 。训练一个 n层神经网络，用于特征映射的内存为 O(n)，需要的正向成本为 。
2.3.2 mxnet-recomputation测试：
Mxnet 在CPU上可以直接调用memonger包，分别由两个优化策略实现CPU内存优化。一是inplace策略，通过模拟图的遍历过程，对每个变量计数，计算当前变量被多少个其他变量使用。当某个变量的计数变成0时，便回收其内存空间。另二是co-share策略，允许两个变量使用但是不能同时写同一段内存空间，对不能并行的变量进行co-share。
Mxnet通过设置环境变量，可以在显存上实现优化算法recomputation，最新的版本无需用户自定义，自动将maxpooling层和bn层设为recomputation的节点。
作者在配置好的annaconda环境下，使用为英特尔 Core i9-9900k 处理器、主频为 3.60 GHz、显卡为 RTX2080TI、显存12G，内存32GiB 的主机上对mxnet-memonger进行了测试，测试结果如表2.3.2.1所示。
表2.3.2.1 mxnet-memonger测试结果
序号	参    数	优化方式	显存内存使用	单次迭代时间（ms）
1	MXNET_BACKWARD_DO_MIRROR=0
MXNET_MEMORY_OPT=0	无	10825 MB	0.783
2	MXNET_BACKWARD_DO_MIRROR=1
MXNET_MEMORY_OPT=0	recomputation	7815MB	0.818
3	MXNET_BACKWARD_DO_MIRROR=1
MXNET_MEMORY_OPT=1	recomputation以及OPT	7287 MB	0.820
4	MXNET_BACKWARD_DO_MIRROR=0
MXNET_MEMORY_OPT=1	OPT	7287 MB	0.819
MXNET_BACKWARD_DO_MIRROR表示是否使用recomputation，如果是则为1，反之则为0。MXNET_MEMORY_OPT表是否使用其他内存优化，如果是则为1，反之则为0。优化方式中OPT表示其他优化。
由数据可知当使用recomputation时，内存优化了27.81%，但是相对于未优化的时间增加了4.47%。序号3和4在多重优化下表现出更少的显存内存使用，但是时间也相对增加，说明以后在迭代时间允许的情况下，可以尝试多重优化并行的优化方法。
2.4 SWAP
SWAP 作为显存优化技术已经广泛应用于深度学习算法中。SWAP是一种基于通用数据流图的显存优化策略，即将主机内存当做一个更大的内存池来克服 显存的大小限制。同时SWAP技术还根据计算图把暂时不参与计算的算子及相关数据放在CPU端，在算子即将需要参与计算时，预先将算子及相关数据从CPU端转移到显存上，进行下一步计算。
由于数据在显存在和CPU中传输较为耗时，尽管swap技术可以使部分计算和传输并行，但仍会带来性能损耗，降低整体的计算速度。
当前，swap技术中使用较为出色的是IBM PowerAI基于swap为tensorflow 开发的Lms算法包。作者对TFLMS算法进行了研究和测试。
2.4.1 TFLMS原理
TensorFlow中用于大模型支持（LMS）的图形编辑库提供了一种训练大模型的方法，它采用用户自定义的图，并自动添加swap-in和swap-out节点，用于将tensors从显存s传输到主机设备，反之亦然。计算图是静态修改的。因此，它需要在会话开始前就完成。
一个计算图或一个神经网络模型被认为是很大的，需要用显存来训练，如果训练一次显存中保留了许多张量，那么则需要更多额外的显存来进行后续的训练。 因此，在训练大网络模型时，经常会出现内存不足的错误。 这本质上是因为在一个计算机中有许多张量具有很长的寿命图表。 此时需要暂时地将显存中的张量发送到CPU，并在必要时将它们发送回显存。
为了将驻留在显存内存中的张量放在CPU内存上，进行了将张量发送到CPU并将其发送回显存的操作。对于 来说，  和 是使用显存执行的。 如式4所示：
	 
（4）
其中上标G代表显存。 此计算可改写为：
	 
（5）
其中上标C代表CPU， 是 的标识函数。
由于 是使用CPU执行的， 的输出张量将在 完成后立即交换到CPU内存中，显存内存被释放。当 被触发时， 的输出张量将被交换到 的显存中。此时将方程5中的函数 称为交换操作。
利用方程5，可以重写一个图，从而减少显存内存消耗。然而，并非所有的边都需要重写。 对于边 ，其中 ， 在 后立即执行。因此，没有必要在这样的边缘交换张量。我们可以为边缘 定义阈值 和图形重写，如果 ，则触发 。
由于两个原因，方程5没有得到优化： CPU的输出张量的交换操作执行的较晚，张量可以被交换和多次交换，因为除了 读取它之外，可能还有多个操作。 必须等待从CPU内存发送到显存内存的张量；
以下是优化方程5的三个规则。 图2.4.1.1显示了每个优化规则获得的计算图。
 图2.4.1.1 每个优化规则的计算图
1）引入交换操作
要在早期交换张量，我们需要一个额外的操作。标识函数可以重写为函数及其反函数的组成，即
	 
（6）
方程5变为：
	 
（7）
因为id也有反函数，所以为f选择id，如果想减少CPU上的内存消耗，可以使用一对编码和解码函数来代替id。
	 
（8）
在方程8中， 将用于将张量交换到设备中，将函数 称为交换操作。值得注意的是，我们必须以良好的顺序手动触发 ；否则， 将在 之后立即执行。为此，必须添加从操作到 的控制边缘。
2）换出操作
一个操作产生的张量通常被多个操作使用，如果张量多次交换到CPU内存，张量则会变得多余。因此，最好将相同张量的交换输出操作合并为单个交换输出操作。
3）换入操作
考虑一种情况，即多个交换操作多次交换张量以进行多个消耗操作。如果张量很大，并且消耗操作彼此接近，然后多次交换张量会带来更多的开销。在这种情况下，最好将交换操作合并为一个交换操作。张量只交换一次，并驻留在显存内存中，由消耗操作重用。例如，在图2.4.1.1中最右边的图中，如果 和 接近并且 很大，然后，我们将 和 融合到一个单独的交换操作中。为了确定两个操作有多接近，我们可以定义它们之间距离的阈值。
4）调度系统
将用于交换操作的控制边缘添加到计算图中，以控制何时触发交换操作，它们对于减少交换张量的通信开销很重要。考虑等式8，操作 中交换的控制操作必须从一组操作中选择，其中 ， ，以保证计算图的正确性。设 为 与v之间的距离。
如果k太小，张量就会在太晚的时候交换， 必须等待张量。如果k太大，张量在太早的时候被交换，张量在设备中保存了很长时间，然后才被 实际使用。
在静态修改计算图的上下文中，引入了两个参数：下界 和上界 来处理选择控制操作。假设使用swap-out操作 和swap-in操作 重写边缘 ：
	 
（9）
以下是两种策略来寻找 的控制操作。
1）直接顺序策略
直接阶策略包括直接使用拓扑排序来获得一组用于控制操作的候选，从目标操作 开始，回到 。下界和上界都是相对于 定义的。候选是指到 的距离在 到 的范围内，并且存在从他们到 的路径。一旦找到满足上述条件的一个操作，算法就停止。
2）链式规则策略
链式规则策略包括从源操作 开始，沿着前向阶段向下寻找相应的后向操作作为控制操作的候选。广度优先搜索用于前向搜索阶段的遍历操作，下界和上界用于限制正向操作的搜索空间。换句话说，下界和上界相对于源操作 定义的。
对于广度优先搜索，我们维护两个开放集 和 ，和一个封闭集 。 包含当前的前向操作， 包含下一级的前向操作(包括 中的所有输出操作)。 包含访问过的操作。从 开始，一旦算法在 到 的范围内，它就会得到当前操作的输出向后操作，然后检查 这些向后操作的有效性。如果有一个有效的操作，它是一个候选然后算法返回它。否则，该算法将进入下一级。
据此，Tensorflow LMS在用户自定义静态图后，根据上述的显存优化算法，对静态图进行修改后，再进行计算图的部署。
2.4.2 TFLMS测试
在tensorflow 1.5.4和更早版本中基于swap技术的包含在tensorflow.contrib模块路径中。Tensorflow2.0发布后，PowerAI发布了基于annaconda的lms算法包，并不再支持TF1版本的Lms算法包。TF2 lms算法包在tf计算图上执行了静态图形修改，引入交换节点和其他图优化。此实现包含在IBM Watson Machine Learning Community Edition 1.6.x版本中。
作者在annaconda环境下，使用英特尔 Core i9-9900k 处理器、主频为 3.60 GHz、显卡为 RTX2080TI、显存12G，内存32G 的主机上对TFLMS进行了测试分析，测试结果如表2.4.2.1所示：
表2.4.2.1 TFLMS测试
序号	批处理时间	Batch	显存内存占用	是否使用LMS	吞吐量（imgs/s）
1	6ms 	10	9650	否	166
2	12ms 	2	8626	否	83
3	4ms	max120	10888	否	250
4	4ms 	120	10920	是	250
5	29ms 	240	10920	是	34
6	19ms 	170	10920	是	53
序号2中，Batch为2但是批处理时间比较长的原因是每次运算都是只使用2个样本，导致程序来回的拿数据，从而造成了时间的流失。由表可知，当数据分析可知，当不使用LMS优化时，Batch设置的越大，批处理时间就越短，Batch最大只能设置到120个样本，但是同时内存也会爆满。当使用LMS后，程序会在显存内存不足时会调用CPU内存，所以由表可知最大Batch（120）为显存的存储上限，且此时无论是否优化，各种性能都不会改变。由于CPU内存有限以及调用内存也会消耗时间，所以使用优化算法后批处理时间会有所增加，吞吐量也会有所减少。
三、总结和展望
本文通过对相关论文的调研及模型实测，对当前业界常用的显存优化算法进行了研究，并重点对Recomputation和swap的实现细节进行了分析和实测。
当使用recomputation时，内存优化了27.81%，但是相对于未优化的单次迭代时间增加了4.47%，所以recomputation是在牺牲少量单次迭代时间的前提下对内存进行了优化。当显存的存储达到上限时，使用LMS后，程序会调用CPU内存，由于CPU内存有限以及调用内存也会消耗时间，所以使用LMS优化算法后批处理时间会有所增加，吞吐量也会有所减少。
可以看出Recomputation和swap通过牺牲部分性能的基础上，减少了显存空间使用。结合实测结果和理论分析，可以看出Recomputation和swap对性能的影响还是较大，有待于进一步提升效率。如何在recomputation的计算流程调度系统将增加的计算量并行到主干计算中，如何在swap中更好的调度，避免cpu\显存数据转移造成主干计算等待将是显存优化领域内的重要课题。另外，如何将多种显存优化进行有效融合也是显存优化领域的一个重要方向。
